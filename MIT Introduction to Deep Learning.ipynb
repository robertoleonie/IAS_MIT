{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Class 1 - Introduction to Deep Learning:**"
      ],
      "metadata": {
        "id": "Ezc9Q9CU8-u4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgjkk8pi87fJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "layer = tf.keras.layers.Dense(units=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "n1 = 2\n",
        "n2 = 3\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(n1),\n",
        "    tf.keras.layers.Dense(n2),\n",
        "    # ...\n",
        "    tf.keras.layers.Dense(2)\n",
        "])"
      ],
      "metadata": {
        "id": "xI38_euU-Q1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(MyDenseLayer, self).__init__()\n",
        "\n",
        "    # Initialize weights and bias\n",
        "    self.W = self.add_weight([input_dim, output_dim])\n",
        "    self.b = self.add_weight([1, output_dim])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Forward propagate the inputs\n",
        "    z = tf.manual(inputs, self.W) + self.b\n",
        "\n",
        "    # Feed through a non-linear activation\n",
        "    output = tf.math.sigmoid(z)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "-jjz9fSe9PHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Cross Entropy Loss\n",
        "y = [[1,2],[3,4]]\n",
        "predicted = [[1,0],[0,1]]\n",
        "loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(y, predicted) )"
      ],
      "metadata": {
        "id": "vjNurKeyACvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error Loss\n",
        "loss = tf.reduce_mean( tf.square(tf.subtract(y, predicted)) )\n",
        "loss = tf.keras.losses.MSE( y, predicted )"
      ],
      "metadata": {
        "id": "70Oe9DJpAxmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent\n",
        "import tensorflow as tf\n",
        "\n",
        "weights = tf.Variable([tf.random.normal()])\n",
        "\n",
        "while True: # loop forever\n",
        "  with tf.GradientTape() as g:\n",
        "    loss = compute_loss(weights)\n",
        "    gradient = g.gradient(loss, weights)\n",
        "\n",
        "  weights = weights - lr * gradient"
      ],
      "metadata": {
        "id": "30U4teWxCOG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent Algorithms\n",
        "tf.keras.optimizers.SGD       # SGD Algorithm\n",
        "tf.keras.optimizers.Adam      # Adam Algorithm\n",
        "tf.keras.optimizers.Adadelta  # Adadelta Algorithm\n",
        "tf.keras.optimizers.Adagrad   # Adagrad Algorithm\n",
        "tf.keras.optimizers.RMSProp   # Root Mean Squared Propagation Algorithm"
      ],
      "metadata": {
        "id": "FBYLDLrDEhl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting it all together\n",
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([...])\n",
        "\n",
        "# pick your favorite optimizer\n",
        "optimizer = tf.keras.optimizer.SGD()  # Can replace with any TensorFlow optimizer!\n",
        "\n",
        "while True: # loop forever\n",
        "  # forward pass through the network\n",
        "  prediction = model(x)\n",
        "  with tf.GradientTape() as tape:\n",
        "    # compute the loss\n",
        "    loss = compute_loss(y, prediction)\n",
        "\n",
        "  # update the weights using the gradient\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "Us6AMV3QFXir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization I: Dropout\n",
        "tf.keras.layers.Dropout(p=0.5)"
      ],
      "metadata": {
        "id": "HWrQAxGIHvVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class 2 - Recurrent Neural Networks, Transformers, and Attention:**"
      ],
      "metadata": {
        "id": "1IQGWfIhKVEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Intuition\n",
        "my_rnn = RNN()\n",
        "hidden_state = [0, 0, 0, 0]\n",
        "\n",
        "sentence = [\"I\", \"love\", \"restaurant\", \"neural\"]\n",
        "\n",
        "for word in sentence:\n",
        "  prediction, hidden_state = my_rnn(word, hidden_state)\n",
        "\n",
        "next_word_prediction = prediction\n",
        "# >>> \"networks!\""
      ],
      "metadata": {
        "id": "Ufz_d1IDPhvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNNs from Scratch\n",
        "class MyRNNCell(tf.keras.layers.Layer):\n",
        "  def __init__(self, rnn_units, input_dim, output_dim):\n",
        "    super(MyRNNCell, self).__init__()\n",
        "\n",
        "    # Initialize weight matrices\n",
        "    self.W_xh = self.add_weight([rnn_units, input_dim])\n",
        "    self.W_hh = self.add_weight([rnn_units, rnn_units])\n",
        "    self.W_hy = self.add_weight([output_dim, rnn_units])\n",
        "\n",
        "    # Initialize hidden state to zeros\n",
        "    self.h = tf.zeros([rnn_units, 1])\n",
        "\n",
        "  def call(self, x):\n",
        "    # Update the hidden state\n",
        "    self.h = tf.math.tanh( self.W_hh * self.h + self.W_xh * x )\n",
        "\n",
        "    # Compute the output\n",
        "    output = self.W_hy * self.h\n",
        "\n",
        "    # Return the current output and hidden state\n",
        "    return output, self.h"
      ],
      "metadata": {
        "id": "PxFTqy1UROK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Implementation in TensorFlow\n",
        "# RNN - Recurrent Neural Networks\n",
        "tf.keras.layers.SimpleRNN(rnn_units)"
      ],
      "metadata": {
        "id": "QuKetbbzS1ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Long Short Term Memory (LSTMs)\n",
        "tf.keras.layers.LSTM(num_units)"
      ],
      "metadata": {
        "id": "lFfSClDHXSo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Task: Sentiment Classification\n",
        "# Input: sequence of words\n",
        "# Output: probability of having positive sentiment\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits(y, predicted)"
      ],
      "metadata": {
        "id": "LuqQSQFSY9D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class 3 - Convolutional Neural Networks:**"
      ],
      "metadata": {
        "id": "xPMXx3GkNphT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.layers.Conv2D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvrhXQkCNqzG",
        "outputId": "a1b1fdcd-811f-4dbd-eaf4-30aee0e70519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras.src.layers.convolutional.conv2d.Conv2D"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.activations.*"
      ],
      "metadata": {
        "id": "fhVYFSaWN909"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.layers.MaxPool2D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOuFmpdCOCMk",
        "outputId": "6a4cec57-5a41-45f7-cf08-739633afe4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras.src.layers.pooling.max_pooling2d.MaxPooling2D"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.layers.Conv2D( filters=d, kernel_size=(h,w), strides=s )"
      ],
      "metadata": {
        "id": "Gh7J_eX1PKUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.layers.ReLU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KO8Vw6UPy6j",
        "outputId": "f9411fc9-7fc5-49e6-8a24-eb2736993d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras.src.layers.activation.relu.ReLU"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.layers.MaxPool2D(\n",
        "    pool_size=(2,2),\n",
        "    strides=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2Znke4zQU3i",
        "outputId": "fefbef38-4fd1-4a4b-87dc-79aae0ceedaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x7a0f4cbae050>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting it all together\n",
        "import tensorflow as tf\n",
        "\n",
        "def generate_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      # first convolutional layer\n",
        "      tf.keras.layers.Conv2D(32, filter_size=3, activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
        "\n",
        "      # second convolutional layer\n",
        "      tf.keras.layers.Conv2D(64, filter_size=3, activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
        "\n",
        "      # fully convolutional layer\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(1024, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')   # 10 outputs\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "7GOKF76XRiJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.layers.Conv2DTranspose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZoepHMDUSmq",
        "outputId": "fbd60633-c7f0-4c91-f277-419fc3548da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras.src.layers.convolutional.conv2d_transpose.Conv2DTranspose"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class 4 - Deep Generative Modeling:**"
      ],
      "metadata": {
        "id": "w6eqfUh9vQaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class 5 - Robust and Trustworthy Deep Learning:**"
      ],
      "metadata": {
        "id": "l8nyEyzuvTwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_ensembles = 5\n",
        "for i in range(num_ensembles) :\n",
        "  model = create_model(...)\n",
        "  model.fit(...)\n",
        "\n",
        "raw_predictions = [models[i].predict(x)\n",
        "  for i in range(num_ensembles)]\n",
        "mu = np.mean(raw_predictions)\n",
        "uncertainty = np.var(raw_predictions)"
      ],
      "metadata": {
        "id": "GhMkkPZFyYv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(T):\n",
        "  forward_passes.append(model(x, dropout=True))\n",
        "mu = np.mean(forward_passes)\n",
        "uncertainty = np.var(forward_passes)"
      ],
      "metadata": {
        "id": "fNrHUNXazaHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = load_dataset()\n",
        "model = build.model(n_layers, n_neurons, ...)\n",
        "model.train(train_data)\n",
        "preds = model.predict(test_data)"
      ],
      "metadata": {
        "id": "WpkLXMKb1mxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = load_dataset()\n",
        "model = build.model(n_layers, n_neurons, ...)\n",
        "model = capsa.HistogramWrapper(model, ...)\n",
        "model.train(train_data)\n",
        "preds, bias = model.predict(test_data)"
      ],
      "metadata": {
        "id": "eSUkMC4r1wUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class 6 - Reinforcement Learning:**"
      ],
      "metadata": {
        "id": "SaaX2sAM26jT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class 7 - Deep Learning New Frontiers:**"
      ],
      "metadata": {
        "id": "KcZgXsMMKqAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class 8 - Text-to-Image Generation:**"
      ],
      "metadata": {
        "id": "ciQBBnOwRe5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class 9 - The Modern Era of Statistics:**"
      ],
      "metadata": {
        "id": "CBSbeRmSRfM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class 10 - The Future of Robot Learning:**"
      ],
      "metadata": {
        "id": "mdAc9OBbIjwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install vista"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfLzQTpeIoYN",
        "outputId": "c862dfaa-1015-430a-85e1-65341ef2b33c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vista\n",
            "  Downloading vista-2.0.6.tar.gz (60 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: vista\n",
            "  Building wheel for vista (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vista: filename=vista-2.0.6-py3-none-any.whl size=73899 sha256=f1caddc2149f0bf4881cbef667168db0cb45fdf8c0ef6e6136feff58971738c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/85/10/6503a8febfee090e49e7a5bfd5cdc4e7edcd941ae20566b80a\n",
            "Successfully built vista\n",
            "Installing collected packages: vista\n",
            "Successfully installed vista-2.0.6\n"
          ]
        }
      ]
    }
  ]
}